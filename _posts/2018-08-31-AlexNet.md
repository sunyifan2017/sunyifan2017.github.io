---
layout:     post
title:      ImageNet Classification with Deep CNN解读
subtitle:   AlexNet
author:     BY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Mac
    - 终端
    - Git

    
---

这是一篇2012的挺老的论文，迄今被引用约 7000  次，被业内普遍视为行业最重要的论文之一。之所以要介绍这篇论文，是因为正是从这篇论文为一个标志，CNN重新进入到大众的视线中，并且火的一塌糊涂。2012年也被称作“深度学习元年”。

首先介绍一下，ImageNet。ImageNet 是一个计算机视觉系统识别项目，是当时世界上图像识别最大的数据库。是美国斯坦福的计算机科学家（李飞飞大神等），模拟人类的识别系统建立的。
ImageNet Large Scale Visual Recognition Challenge (ILSVRC)是ImageNet的图像识别竞赛。ILSVRC主要评价算法在大尺度上对物体检测和图像分类的效果。PS. ILSVRC 2017 已是最后一届举办。


# Introduction

本篇论文主要的贡献：提出了深度卷积神经网络模型Alexnet，用于对ImageNet图像数据集进行分类，在2012年ILSVRC分类比赛取得top-5错误率15.3%获得冠军。Alexnet神经网络模型包含5个convolutional layers 卷基层，和3个 fully-connected layers 全联接层；并且他们发现，卷基层和全连接层的深度只有是5和3时，此模型表现最好。

# The Dataset

ImageNet: 1500万张图片，2.2万类。
ISLVRC: ImageNet的子集。训练集1281167张图片+标签，验证集50000张图片+标签。


因为ImageNet包含各种像素的图像，本文作者将图片下取样为统一像素。除此之外没有对图片进行其他处理，所以每个像素点依然包含最原始的RGB三色道(channel)。

# The	Architecture

模型介绍是本篇论文的重点（参赛后总结文，数学推导相对较少）。在正式介绍模型之前，需要了解一下本模型中几个重要特征。

## 3.1 ReLu Nonlinearity

在各种模型中，比较常见的激励函数包括 f(x) = tanh(x),  f(x) = (1 + e􀀀x)􀀀1。就梯度下降所需要的训练时间而言，同样是非线性的ReLu：f(x) = max(0; x)却远远快于上述两个常见激励函数。

上图中，横坐标为Epochs， 纵坐标为训练错误率；实线为ReLu，虚线为 tanh；可以明显看出，ReLu明显快于tanh达到错误率小于25%。

## 3.2 Training on  multiple GPUs

这是因为碍于2010年GPU的最大memory仅有3GB，对于训练120万个样本实在是心有余而力不足。因此此文作者提出并行使用2个GPU来应对巨大的计算量。

## 3.3  Local Response Normalization

局部响应归一化LRN。（对于局部响应归一化，始终是褒贬不一，2015年Very Deep COnvolutinal Networks for Large-Scale Image Recognition 一文中提发哦LRN基本没什么用。）

局部响应归一化的概念源于神经生物学里的 侧抑制 lateral inhibitio，指的是被激活的神经元会抑制相邻的神经元。我们知道归一化 normalization的目的就是“抑制”，而局部响应归一化就是借鉴啦 神经生物学中的侧抑制。即 CNN中的每个神经元之间彼此竞争，影响大的神经元会抑制影响小的神经元，达到防止过拟合的目的。


图片


公式看上去比较复杂，但理解起来非常简单。a表示第i个核在位置（x,y）运用激活函数ReLU后的输出，n是同一位置上临近的kernal map的数目，N是kernal的总数。参数K,n,alpha，belta都是超参数，一般设置k=2,n=5,aloha=1*e-4,beta=0.75。

## 3.4  Overlapping pooling

 https://adriancolyer.files.wordpress.com/2017/03/overlapping-pooling.jpeg
说到Overlapping pooling就一定会涉及1个概念 stride 步长。卷积核移动的步长（stride）小于卷积核的边长（一般为正方行）时，变会出现卷积核与原始输入矩阵作用范围在区域上的重叠（overlap），卷积核移动的步长（stride）与卷积核的边长相一致时，不会出现重叠现象。数学的方式解释为，设步长为s，卷积核变长为z。当s=z时，不重叠；当 s<z时，重叠。

## 3.5 Overall Architecture 

现在我们终于准备进入正题-----此CNN的模型结构。

图片

这就是大名鼎鼎的AlexNet。
我们一层一层的介绍。
输入：图片224 X 224 X 3。
第一层：卷积层。 96个11 X 11 X 3 的kernels，伴以步长为4，并进行max pooling最大池化则可以得到96张特征图 feature map。
第二层：卷积层。 256个5 X 5X 48的kernels，并进行max pooling最大池化则可以得到96张特征图 feature map。
第三层：卷积层。 384个3 X 3X 256的kernels，并进行max pooling最大池化则可以得到96张特征图 feature map。
第四层：卷积层。 192个3 X 3 X 192的kernels，并进行max pooling最大池化则可以得到96张特征图 feature map。
第五层：卷积层。 256个3 X 3 X 192的kernels，并进行max pooling最大池化则可以得到96张特征图 feature map。
第六层：全连接层。包含4096个神经元。
第七层：全连接层。包含4096个神经元。
第八层：全连接层。包含4096个神经元。

Reducing Overfitting

此神经网络即包含6000万个参数。即使训练集样本数量很大，但是这么多参数下，还是有出现过拟合的可能性。为了避免过拟合，此文作者提出2种方式来应对。

4.1 Data Augmentation

最简单也是最常用的避免过拟合的方法就是人为增大训练集。此处有2种方法增大数据集。

 将原像素为256 X 256的图片任意截取成 224 X 224大小;及其水平翻转后的图像作为训练数据。
交换图像RGB的值，我们知道图片颜色有三原色RGB叠加而成，比如(0,0,0)为黑色；文章先对RGB像素值作主成分分析，然后对每张训练图像的像素值，加上主成分乘对应特征值和均值为1，标准差0.1的随机值的积：

图片

4.2 Dropout

第二种避免过拟合的方法是 Dropout。顾名思义Dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。则在训练过程中，每一次我们都会有一个不同的模型结果；但是所有的这些模型又共享同一组参数。因为每一个神经元不能过于依赖其他神经元，故可以减少神经元之间的互相适应 (co-adaptation) 。

Result

在ILSVRC-2010数据集上网络取得了top-1和top-5错误率37.5%和17.0%。


文章评价

总体来说这篇论文中涉及的数学公式较少，数学依据也相对较少。但是它使神经网络黑箱的形式重新登上机器学习的舞台。而且本篇论文中的神经网络结构对日后包括从GAN等更复杂的DNN都具有深刻的影响。


