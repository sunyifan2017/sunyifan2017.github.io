---
layout:     post
title:      Ng机器学习前半部分笔记总结(1)线性回归
subtitle:   拖拖拉拉的才想来总结
date:       2018-08-21
author:     SUN YIFAN
header-img: img/post-bg-desk.jpg
catalog: true
tags:
    - Machine learning
    - Ng Andrew
    - 复习
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

#### 写在前面的话。
好久没有学习啦，从头捡一下，NG 的 machine learning 前半部分复习总结一下,如果有经历的话，再加入一点西瓜书和统计学习方法的推导。

# Linear Regression 线性回归

$$x=(x_1,x_2,x_3,...,x_d)$$
其中,$x_i$ 代表各个属性。


线性模型(linear model)
$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$
采用向量f(**x**)=**wx**+b表示。


#### TIP
```
对于离散属性，若属性间存在 序 order,可通过连续化转化为连续值。

比如身高“高”，“矮”，可以转化为{1.0，0.0}。

若属性间不存在序关系，比如有k种情况，则转化为k维向量。

ex，“西瓜”“南瓜”“冬瓜”-->(0,0,1),(0,1,0),(1,0,0).
```


目的：学得f(**x<sub>i</sub>**)=**wx<sub>i</sub>**+b，使得f(**x<sub>i</sub>**)-->y<sub>i</sub>.

参数**w**和b的估计，通过f(**x<sub>i</sub>**)-->y<sub>i</sub>来判断。

引进均方误差：($w^*,b^*$),带 *的意思是($w,b$)的解。

#### TIP 引进均方误差的原因。

      均方误差的的几何意义优秀，它对欧氏距离(Eudidean Distance)。
      基于均方误差最小化进行模型求解的方法即被称为 最小二乘法(least square method)。
      在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离距离最小。


#### 下面推导一下$d=1$时，求解$w,b$。

$E_(w,b)$ 是关于$w,b$的凸函数。当它关于$w,b$的导数为0时，得到$w,b$的最优解。

      简单说，就是整个函数的形状是个大写的U。二阶导数非负。

$d=1$, 则$f(x)=wx+b$。

$$E_(w,b)=\sum_{k=1}^m (y_i-wx_i-b)^2$$
可知
$$\frac{\partial f(x, y)}{\partial w}\ = 2(w \sum_i^m x_i^2-\sum_i^m(y_i-b)x_i)=0$$
$$\frac{\partial f(x, y)}{\partial b}\ = 2(mb-\sum_i^m(y_i-wx_i))=0$$

 由（2）式得：
 $$b=\frac{1}{m}\ \sum_i^m(y_i-wx_i)\$$
 将b带入（1）式：
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ + b\sum_i^mx_i\ = 0$$

 其中
 $$b\sum_i^mx_i\ = \frac{1}{m}\ \sum_i^m(y_i-wx_i)\sum_i^mx_i\$$
 $$=\frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\$$

 带回到上式：
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ + \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\ = 0$$
 提取$w$:
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ - \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ $$

 其中：
 $$ \frac{1}{m}\ \sum_i^m x_i\ = \bar{x}$$
 则
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \sum_i^m y_i\ \bar{x}$$
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_i(x_i-\bar{x})\ $$

 可推出：

 $$w= \frac { \sum_i^m y_i(x_i- \bar x\ )\ }{ \sum_i^m x_i^2\ - \frac {1}{m}\sum_i^m x_i^2}\$$


```
在Ng的这门课里用到的语言是Octave.GNU Octave是一个类MATLAB软件。
其在GNU开放软件框架下， 可以代替MATLAB的大多数功能，但不包括Simulink组件功能。
```

#### 下面的代码会用到Octave。

```
%% ==================== Part 1: Basic Function ====================
% Complete warmUpExercise.m
fprintf('Running warmUpExercise ... \n');
fprintf('5x5 Identity Matrix: \n');
warmUpExercise()

fprintf('Program paused. Press enter to continue.\n');
pause;
```

warmUpExercise这个热身函数，要求是返回一个5x5的矩阵，return the 5x5 identify Matrix.

```
function A=warmUpExercise()
A=[];
A=eye(5);
end
```

这里需要注意的点，
* 开头处`function` `A` 和 `warmUpExercise` 分别代表返回值和函数名称。
* `eye(n)` 是octave自带函数，生成一个，nxn的矩阵。对角线为1，其余为0.

生成结果为
$$\left( \begin{array}{ccc}
    1 & 0 & 0 & 0 & 0\\
    0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0\\
    0 & 0 & 0 & 1 & 0\\
    0 & 0 & 0 & 0 & 1\\
\end{array}\right)$$

```
%% ======================= Part 2: Plotting =======================
fprintf('Plotting Data ...\n')
data = load('ex1data1.txt');
X = data(:, 1); y = data(:, 2);
m = length(y); % number of training examples

% Plot Data
% Note: You have to complete the code in plotData.m
plotData(X, y);

fprintf('Program paused. Press enter to continue.\n');
pause;
```

这里需要注意的点，
* `load`是octave自带命令，载入文件。
* ex1ex1data1.txt是一组如下图所示数据。
![ex1ex1data1](img/Ng_machine_learning_1/ex1dara1.png)
*`data(:,1)`表示取第一列；`data(:,2)`同理表示取第二列。
* `length()`表示向量的长度，即训练集里样本个数。
* 画数据，引用`plotData()`函数。

```
function plotData(x,y)

plot(x,y,'rx',"MarkerSize",10);
ylabel("profit in $10000s");
xlabel("population of city in 10000s");
```
这里需要注意的点，
* `plot`是octave自带函数，用来画图，‘rx’代表绘图点用红色叉x，“MarkerSize”表示绘图点尺寸。
* xlabel,ylabel分别代表x坐标轴和y坐标轴标签。

```
%% =================== Part 3: Cost and Gradient descent ===================

X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
theta = zeros(2, 1); % initialize fitting parameters

% Some gradient descent settings
iterations = 1500;
alpha = 0.01;

fprintf('\nTesting the cost function ...\n')
% compute and display initial cost
J = computeCost(X, y, theta);
fprintf('With theta = [0 ; 0]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 32.07\n');

% further testing of the cost function
J = computeCost(X, y, [-1 ; 2]);
fprintf('\nWith theta = [-1 ; 2]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 54.24\n');

fprintf('Program paused. Press enter to continue.\n');
pause;

fprintf('\nRunning Gradient Descent ...\n')
% run gradient descent
theta = gradientDescent(X, y, theta, alpha, iterations);

% print theta to screen
fprintf('Theta found by gradient descent:\n');
fprintf('%f\n', theta);
fprintf('Expected theta values (approx)\n');
fprintf(' -3.6303\n  1.1664\n\n');

% Plot the linear fit
hold on; % keep previous plot visible
plot(X(:,2), X*theta, '-')
legend('Training data', 'Linear regression')
hold off % don't overlay any more plots on this figure

% Predict values for population sizes of 35,000 and 70,000
predict1 = [1, 3.5] *theta;
fprintf('For population = 35,000, we predict a profit of %f\n',...
    predict1*10000);
predict2 = [1, 7] * theta;
fprintf('For population = 70,000, we predict a profit of %f\n',...
    predict2*10000);

fprintf('Program paused. Press enter to continue.\n');
pause;

```

这里需要注意的点，
* data(:,1)是mx1的向量，为了对应$w,b$合并成的一个参数    $\theta$  ,即变为mx2的矩阵，需要在data(:,1)前面加一列m行的**1**向量。这就是为什么X=[ones(m,1),data(:,1)] 的原因。
*  $\theta$ = ${w \choose b}$,即 theta=zeros(2,1),2x1的零向量。



```
function J=computeCost(X,y,theta)

m=length(y);
J=sum((X*theta-y).^2)/(2*m);

end
```

这里需要注意的点，
* J即损失函数，使用的是均方误差.
$$ J=\frac { \sum_i^m (X*\theta - y\ )^2\ }{ 2m}\$$


其实我们便携的这个代价函数是为了最为重要的gradientDescent梯度下降函数做准备的。

```
function [theta,J_history]=gradientDescent(X,y,theta,alpha,num_iters)

for i=1:num_iters:
temp0=theta(1,1)-sum(X*theta-y)/m*alpha;
temp1=theta(2,1)-sum((X*theta-y).*X(:,2))/m*alpha;
theta(1,1)=temp0;
theta(2,1)=temp1;

J_history(iter) = computeCost(X, y, theta);

end
```

这里需要注意的点，
* temp0和theta(1,1)即是$b$,temp1 和theta(2,1)即为$w$,

```

%% ============= Part 4: Visualizing J(theta_0, theta_1) =============
fprintf('Visualizing J(theta_0, theta_1) ...\n')

% Grid over which we will calculate J
theta0_vals = linspace(-10, 10, 100);
theta1_vals = linspace(-1, 4, 100);

% initialize J_vals to a matrix of 0's
J_vals = zeros(length(theta0_vals), length(theta1_vals));

% Fill out J_vals
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
	  t = [theta0_vals(i); theta1_vals(j)];
	  J_vals(i,j) = computeCost(X, y, t);
    end
end
```
