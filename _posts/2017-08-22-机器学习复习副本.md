---
layout:     post
title:      Ngæœºå™¨å­¦ä¹ å‰åŠéƒ¨åˆ†ç¬”è®°æ€»ç»“
subtitle:   æ‹–æ‹–æ‹‰æ‹‰çš„æ‰æƒ³æ¥æ€»ç»“
date:       2018-08-21
author:     SUN YIFAN
header-img: img/post-bg-review.jpg
catalog: true
tags:
    - Machine learning
    - Ng Andrew
    - å¤ä¹ 
---
$R^n$
$X$
$Y$


$x \in X$
$y \in Y$

$x \in \mathcal{X} $
$y \in \mathcal{Y} $

$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$

$N$

$(x_i,y_i)$

$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$
#### å†™åœ¨å‰é¢çš„è¯ã€‚
å¥½ä¹…æ²¡æœ‰å­¦ä¹ å•¦ï¼Œä»å¤´æ¡ä¸€ä¸‹ï¼ŒNG çš„ machine learning å‰åŠéƒ¨åˆ†å¤ä¹ æ€»ç»“ä¸€ä¸‹,å¦‚æœæœ‰ç»å†çš„è¯ï¼Œå†åŠ å…¥ä¸€ç‚¹è¥¿ç“œä¹¦å’Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•çš„æ¨å¯¼ã€‚

# Linear Regression çº¿æ€§å›å½’

$$x=(x_1,x_2,x_3,...,x_d)$$
å…¶ä¸­,$x_i$ ä»£è¡¨å„ä¸ªå±æ€§ã€‚


çº¿æ€§æ¨¡å‹(linear model)
$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$
é‡‡ç”¨å‘é‡f(**x**)=**wx**+bè¡¨ç¤ºã€‚


#### TIP
```
å¯¹äºç¦»æ•£å±æ€§ï¼Œè‹¥å±æ€§é—´å­˜åœ¨ åº order,å¯é€šè¿‡è¿ç»­åŒ–è½¬åŒ–ä¸ºè¿ç»­å€¼ã€‚

æ¯”å¦‚èº«é«˜â€œé«˜â€ï¼Œâ€œçŸ®â€ï¼Œå¯ä»¥è½¬åŒ–ä¸º{1.0ï¼Œ0.0}ã€‚

è‹¥å±æ€§é—´ä¸å­˜åœ¨åºå…³ç³»ï¼Œæ¯”å¦‚æœ‰kç§æƒ…å†µï¼Œåˆ™è½¬åŒ–ä¸ºkç»´å‘é‡ã€‚

exï¼Œâ€œè¥¿ç“œâ€â€œå—ç“œâ€â€œå†¬ç“œâ€-->(0,0,1),(0,1,0),(1,0,0).
```


ç›®çš„ï¼šå­¦å¾—f(**x<sub>i</sub>**)=**wx<sub>i</sub>**+bï¼Œä½¿å¾—f(**x<sub>i</sub>**)-->y<sub>i</sub>.

å‚æ•°**w**å’Œbçš„ä¼°è®¡ï¼Œé€šè¿‡f(**x<sub>i</sub>**)-->y<sub>i</sub>æ¥åˆ¤æ–­ã€‚

å¼•è¿›å‡æ–¹è¯¯å·®ï¼š($w^*,b^*$),å¸¦ *çš„æ„æ€æ˜¯($w,b$)çš„è§£ã€‚

#### TIP å¼•è¿›å‡æ–¹è¯¯å·®çš„åŸå› ã€‚

      å‡æ–¹è¯¯å·®çš„çš„å‡ ä½•æ„ä¹‰ä¼˜ç§€ï¼Œå®ƒå¯¹æ¬§æ°è·ç¦»(Eudidean Distance)ã€‚
      åŸºäºå‡æ–¹è¯¯å·®æœ€å°åŒ–è¿›è¡Œæ¨¡å‹æ±‚è§£çš„æ–¹æ³•å³è¢«ç§°ä¸º æœ€å°äºŒä¹˜æ³•(least square method)ã€‚
      åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæœ€å°äºŒä¹˜æ³•å°±æ˜¯è¯•å›¾æ‰¾åˆ°ä¸€æ¡ç›´çº¿ï¼Œä½¿æ‰€æœ‰æ ·æœ¬åˆ°ç›´çº¿ä¸Šçš„æ¬§æ°è·ç¦»è·ç¦»æœ€å°ã€‚


#### ä¸‹é¢æ¨å¯¼ä¸€ä¸‹$d=1$æ—¶ï¼Œæ±‚è§£$w,b$ã€‚

$E_(w,b)$ æ˜¯å…³äº$w,b$çš„å‡¸å‡½æ•°ã€‚å½“å®ƒå…³äº$w,b$çš„å¯¼æ•°ä¸º0æ—¶ï¼Œå¾—åˆ°$w,b$çš„æœ€ä¼˜è§£ã€‚

      ç®€å•è¯´ï¼Œå°±æ˜¯æ•´ä¸ªå‡½æ•°çš„å½¢çŠ¶æ˜¯ä¸ªå¤§å†™çš„Uã€‚äºŒé˜¶å¯¼æ•°éè´Ÿã€‚

$d=1$, åˆ™$f(x)=wx+b$ã€‚

$$E_(w,b)=\sum_{k=1}^m (y_i-wx_i-b)^2$$
å¯çŸ¥
$$\frac{\partial f(x, y)}{\partial w}\ = 2(w \sum_i^m x_i^2-\sum_i^m(y_i-b)x_i)=0$$
$$\frac{\partial f(x, y)}{\partial b}\ = 2(mb-\sum_i^m(y_i-wx_i))=0$$

 ç”±ï¼ˆ2ï¼‰å¼å¾—ï¼š
 $$b=\frac{1}{m}\ \sum_i^m(y_i-wx_i)\$$
 å°†bå¸¦å…¥ï¼ˆ1ï¼‰å¼ï¼š
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ + b\sum_i^mx_i\ = 0$$

 å…¶ä¸­
 $$b\sum_i^mx_i\ = \frac{1}{m}\ \sum_i^m(y_i-wx_i)\sum_i^mx_i\$$
 $$=\frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\$$

 å¸¦å›åˆ°ä¸Šå¼ï¼š
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ + \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\ = 0$$
 æå–$w$:
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ - \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ $$

 å…¶ä¸­ï¼š
 $$ \frac{1}{m}\ \sum_i^m x_i\ = \bar{x}$$
 åˆ™
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \sum_i^m y_i\ \bar{x}$$
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_i(x_i-\bar{x})\ $$

 å¯æ¨å‡ºï¼š

 $$w= \frac { \sum_i^m y_i(x_i- \bar x\ )\ }{ \sum_i^m x_i^2\ - \frac {1}{m}\sum_i^m x_i^2}\$$


```
åœ¨Ngçš„è¿™é—¨è¯¾é‡Œç”¨åˆ°çš„è¯­è¨€æ˜¯Octave.GNU Octaveæ˜¯ä¸€ä¸ªç±»MATLABè½¯ä»¶ã€‚
å…¶åœ¨GNUå¼€æ”¾è½¯ä»¶æ¡†æ¶ä¸‹ï¼Œ å¯ä»¥ä»£æ›¿MATLABçš„å¤§å¤šæ•°åŠŸèƒ½ï¼Œä½†ä¸åŒ…æ‹¬Simulinkç»„ä»¶åŠŸèƒ½ã€‚
```

#### ä¸‹é¢çš„ä»£ç ä¼šç”¨åˆ°Octaveã€‚

```
%% ==================== Part 1: Basic Function ====================
% Complete warmUpExercise.m
fprintf('Running warmUpExercise ... \n');
fprintf('5x5 Identity Matrix: \n');
warmUpExercise()

fprintf('Program paused. Press enter to continue.\n');
pause;
```

warmUpExerciseè¿™ä¸ªçƒ­èº«å‡½æ•°ï¼Œè¦æ±‚æ˜¯è¿”å›ä¸€ä¸ª5x5çš„çŸ©é˜µï¼Œreturn the 5x5 identify Matrix.

```
function A=warmUpExercise()
A=[];
A=eye(5);
end
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* å¼€å¤´å¤„`function` `A` å’Œ `warmUpExercise` åˆ†åˆ«ä»£è¡¨è¿”å›å€¼å’Œå‡½æ•°åç§°ã€‚
* `eye(n)` æ˜¯octaveè‡ªå¸¦å‡½æ•°ï¼Œç”Ÿæˆä¸€ä¸ªï¼Œnxnçš„çŸ©é˜µã€‚å¯¹è§’çº¿ä¸º1ï¼Œå…¶ä½™ä¸º0.

ç”Ÿæˆç»“æœä¸º
$$\left( \begin{array}{ccc}
    1 & 0 & 0 & 0 & 0\\
    0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0\\
    0 & 0 & 0 & 1 & 0\\
    0 & 0 & 0 & 0 & 1\\
\end{array}\right)$$

```
%% ======================= Part 2: Plotting =======================
fprintf('Plotting Data ...\n')
data = load('ex1data1.txt');
X = data(:, 1); y = data(:, 2);
m = length(y); % number of training examples

% Plot Data
% Note: You have to complete the code in plotData.m
plotData(X, y);

fprintf('Program paused. Press enter to continue.\n');
pause;
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* `load`æ˜¯octaveè‡ªå¸¦å‘½ä»¤ï¼Œè½½å…¥æ–‡ä»¶ã€‚
* ex1ex1data1.txtæ˜¯ä¸€ç»„å¦‚ä¸‹å›¾æ‰€ç¤ºæ•°æ®ã€‚
![ex1ex1data1](img/Ng_machine_learning_1/ex1dara1.png)
*`data(:,1)`è¡¨ç¤ºå–ç¬¬ä¸€åˆ—ï¼›`data(:,2)`åŒç†è¡¨ç¤ºå–ç¬¬äºŒåˆ—ã€‚
* `length()`è¡¨ç¤ºå‘é‡çš„é•¿åº¦ï¼Œå³è®­ç»ƒé›†é‡Œæ ·æœ¬ä¸ªæ•°ã€‚
* ç”»æ•°æ®ï¼Œå¼•ç”¨`plotData()`å‡½æ•°ã€‚

```
function plotData(x,y)

plot(x,y,'rx',"MarkerSize",10);
ylabel("profit in $10000s");
xlabel("population of city in 10000s");
```
è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* `plot`æ˜¯octaveè‡ªå¸¦å‡½æ•°ï¼Œç”¨æ¥ç”»å›¾ï¼Œâ€˜rxâ€™ä»£è¡¨ç»˜å›¾ç‚¹ç”¨çº¢è‰²å‰xï¼Œâ€œMarkerSizeâ€è¡¨ç¤ºç»˜å›¾ç‚¹å°ºå¯¸ã€‚
* xlabel,ylabelåˆ†åˆ«ä»£è¡¨xåæ ‡è½´å’Œyåæ ‡è½´æ ‡ç­¾ã€‚

```
%% =================== Part 3: Cost and Gradient descent ===================

X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
theta = zeros(2, 1); % initialize fitting parameters

% Some gradient descent settings
iterations = 1500;
alpha = 0.01;

fprintf('\nTesting the cost function ...\n')
% compute and display initial cost
J = computeCost(X, y, theta);
fprintf('With theta = [0 ; 0]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 32.07\n');

% further testing of the cost function
J = computeCost(X, y, [-1 ; 2]);
fprintf('\nWith theta = [-1 ; 2]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 54.24\n');

fprintf('Program paused. Press enter to continue.\n');
pause;

fprintf('\nRunning Gradient Descent ...\n')
% run gradient descent
theta = gradientDescent(X, y, theta, alpha, iterations);

% print theta to screen
fprintf('Theta found by gradient descent:\n');
fprintf('%f\n', theta);
fprintf('Expected theta values (approx)\n');
fprintf(' -3.6303\n  1.1664\n\n');

% Plot the linear fit
hold on; % keep previous plot visible
plot(X(:,2), X*theta, '-')
legend('Training data', 'Linear regression')
hold off % don't overlay any more plots on this figure

% Predict values for population sizes of 35,000 and 70,000
predict1 = [1, 3.5] *theta;
fprintf('For population = 35,000, we predict a profit of %f\n',...
    predict1*10000);
predict2 = [1, 7] * theta;
fprintf('For population = 70,000, we predict a profit of %f\n',...
    predict2*10000);

fprintf('Program paused. Press enter to continue.\n');
pause;

```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* data(:,1)æ˜¯mx1çš„å‘é‡ï¼Œä¸ºäº†å¯¹åº”$w,b$åˆå¹¶æˆçš„ä¸€ä¸ªå‚æ•°    $\theta$  ,å³å˜ä¸ºmx2çš„çŸ©é˜µï¼Œéœ€è¦åœ¨data(:,1)å‰é¢åŠ ä¸€åˆ—mè¡Œçš„**1**å‘é‡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆX=[ones(m,1),data(:,1)] çš„åŸå› ã€‚
*  $\theta$ = ${w \choose b}$,å³ theta=zeros(2,1),2x1çš„é›¶å‘é‡ã€‚



```
function J=computeCost(X,y,theta)

m=length(y);
J=sum((X*theta-y).^2)/(2*m);

end
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* Jå³æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨çš„æ˜¯å‡æ–¹è¯¯å·®.
$$ J=\frac { \sum_i^m (X*\theta - y\ )^2\ }{ 2m}\$$


å…¶å®æˆ‘ä»¬ä¾¿æºçš„è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯ä¸ºäº†æœ€ä¸ºé‡è¦çš„gradientDescentæ¢¯åº¦ä¸‹é™å‡½æ•°åšå‡†å¤‡çš„ã€‚

```
function [theta,J_history]=gradientDescent(X,y,theta,alpha,num_iters)

for i=1:num_iters:
temp0=theta(1,1)-sum(X*theta-y)/m*alpha;
temp1=theta(2,1)-sum((X*theta-y).*X(:,2))/m*alpha;
theta(1,1)=temp0;
theta(2,1)=temp1;

J_history(iter) = computeCost(X, y, theta);

end
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹ï¼Œ
* temp0å’Œtheta(1,1)å³æ˜¯$b$,temp1 å’Œtheta(2,1)å³ä¸º$w$,

```

%% ============= Part 4: Visualizing J(theta_0, theta_1) =============
fprintf('Visualizing J(theta_0, theta_1) ...\n')

% Grid over which we will calculate J
theta0_vals = linspace(-10, 10, 100);
theta1_vals = linspace(-1, 4, 100);

% initialize J_vals to a matrix of 0's
J_vals = zeros(length(theta0_vals), length(theta1_vals));

% Fill out J_vals
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
	  t = [theta0_vals(i); theta1_vals(j)];
	  J_vals(i,j) = computeCost(X, y, t);
    end
end
```

# Logistic Regression/classification çº¿æ€§å›å½’/åˆ†ç±»
ç»™å®šç”±dä¸ªå±æ€§æè¿°çš„å®ä¾‹ **x**=( x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>d</sub> )ï¼Œå…¶ä¸­x<sub>i</sub>æ˜¯**x**åœ¨ç¬¬iä¸ªå±æ€§ä¸Šçš„å–å€¼ã€‚


#### classification åˆ†ç±»
*  $y\in ({0,1})$ï¼Œ **0**:negative class
					 **1**:positive class

*  **y** \in {0,1,2,3}

##### ä¾‹å­ğŸŒ°
