---
layout:     post
title:      Ng机器学习前半部分笔记总结
subtitle:   拖拖拉拉的才想来总结
date:       2018-08-21
author:     SUN YIFAN
header-img: img/post-bg-review.jpg
catalog: true
tags:
    - Machine learning
    - Ng Andrew
    - 复习
---
$R^n$
$X$
$Y$


$x \in X$
$y \in Y$

$x \in \mathcal{X} $
$y \in \mathcal{Y} $

$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$

$N$

$(x_i,y_i)$

$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$
#### 写在前面的话。
好久没有学习啦，从头捡一下，NG 的 machine learning 前半部分复习总结一下,如果有经历的话，再加入一点西瓜书和统计学习方法的推导。

# Linear Regression 线性回归

$$x=(x_1,x_2,x_3,...,x_d)$$
其中,$x_i$ 代表各个属性。


线性模型(linear model)
$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$
采用向量f(**x**)=**wx**+b表示。


#### TIP
```
对于离散属性，若属性间存在 序 order,可通过连续化转化为连续值。

比如身高“高”，“矮”，可以转化为{1.0，0.0}。

若属性间不存在序关系，比如有k种情况，则转化为k维向量。

ex，“西瓜”“南瓜”“冬瓜”-->(0,0,1),(0,1,0),(1,0,0).
```


目的：学得f(**x<sub>i</sub>**)=**wx<sub>i</sub>**+b，使得f(**x<sub>i</sub>**)-->y<sub>i</sub>.

参数**w**和b的估计，通过f(**x<sub>i</sub>**)-->y<sub>i</sub>来判断。

引进均方误差：($w^*,b^*$),带 *的意思是($w,b$)的解。

#### TIP 引进均方误差的原因。

      均方误差的的几何意义优秀，它对欧氏距离(Eudidean Distance)。
      基于均方误差最小化进行模型求解的方法即被称为 最小二乘法(least square method)。
      在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离距离最小。


#### 下面推导一下$d=1$时，求解$w,b$。

$E_(w,b)$ 是关于$w,b$的凸函数。当它关于$w,b$的导数为0时，得到$w,b$的最优解。

      简单说，就是整个函数的形状是个大写的U。二阶导数非负。

$d=1$, 则$f(x)=wx+b$。

$$E_(w,b)=\sum_{k=1}^m (y_i-wx_i-b)^2$$
可知
$$\frac{\partial f(x, y)}{\partial w}\ = 2(w \sum_i^m x_i^2-\sum_i^m(y_i-b)x_i)=0$$
$$\frac{\partial f(x, y)}{\partial b}\ = 2(mb-\sum_i^m(y_i-wx_i))=0$$

 由（2）式得：
 $$b=\frac{1}{m}\ \sum_i^m(y_i-wx_i)\$$
 将b带入（1）式：
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ - b\sum_i^mx_i\ = 0$$

 其中
 $$b\sum_i^mx_i\ = \frac{1}{m}\ \sum_i^m(y_i-wx_i)\sum_i^mx_i\$$
 $$=\frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\$$

 带回到上式：
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ - \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\ = 0$$
 提取$w$:
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ $$

 其中：
 $$ \frac{1}{m}\ \sum_i^m x_i\ = \bar{x}$$
 则
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \sum_i^m y_i\ \bar{x}$$
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_i(x_i+\bar{x})\ $$

 可推出：
 



# Logistic Regression/classification 线性回归/分类
给定由d个属性描述的实例 **x**=( x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>d</sub> )，其中x<sub>i</sub>是**x**在第i个属性上的取值。


#### classification 分类
*  $y\in ({0,1})$， **0**:negative class
					 **1**:positive class

*  **y** \in {0,1,2,3}

##### 例子🌰
