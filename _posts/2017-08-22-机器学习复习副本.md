---
layout:     post
title:      Ngæœºå™¨å­¦ä¹ å‰åŠéƒ¨åˆ†ç¬”è®°æ€»ç»“
subtitle:   æ‹–æ‹–æ‹‰æ‹‰çš„æ‰æƒ³æ¥æ€»ç»“
date:       2018-08-21
author:     SUN YIFAN
header-img: img/post-bg-review.jpg
catalog: true
tags:
    - Machine learning
    - Ng Andrew
    - å¤ä¹ 
---
$R^n$
$X$
$Y$


$x \in X$
$y \in Y$

$x \in \mathcal{X} $
$y \in \mathcal{Y} $

$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$

$N$

$(x_i,y_i)$

$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$
#### å†™åœ¨å‰é¢çš„è¯ã€‚
å¥½ä¹…æ²¡æœ‰å­¦ä¹ å•¦ï¼Œä»å¤´æ¡ä¸€ä¸‹ï¼ŒNG çš„ machine learning å‰åŠéƒ¨åˆ†å¤ä¹ æ€»ç»“ä¸€ä¸‹,å¦‚æœæœ‰ç»å†çš„è¯ï¼Œå†åŠ å…¥ä¸€ç‚¹è¥¿ç“œä¹¦å’Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•çš„æ¨å¯¼ã€‚

# Linear Regression çº¿æ€§å›å½’

$$x=(x_1,x_2,x_3,...,x_d)$$
å…¶ä¸­,$x_i$ ä»£è¡¨å„ä¸ªå±æ€§ã€‚


çº¿æ€§æ¨¡å‹(linear model)
$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$
é‡‡ç”¨å‘é‡f(**x**)=**wx**+bè¡¨ç¤ºã€‚


#### TIP
```
å¯¹äºç¦»æ•£å±æ€§ï¼Œè‹¥å±æ€§é—´å­˜åœ¨ åº order,å¯é€šè¿‡è¿ç»­åŒ–è½¬åŒ–ä¸ºè¿ç»­å€¼ã€‚

æ¯”å¦‚èº«é«˜â€œé«˜â€ï¼Œâ€œçŸ®â€ï¼Œå¯ä»¥è½¬åŒ–ä¸º{1.0ï¼Œ0.0}ã€‚

è‹¥å±æ€§é—´ä¸å­˜åœ¨åºå…³ç³»ï¼Œæ¯”å¦‚æœ‰kç§æƒ…å†µï¼Œåˆ™è½¬åŒ–ä¸ºkç»´å‘é‡ã€‚

exï¼Œâ€œè¥¿ç“œâ€â€œå—ç“œâ€â€œå†¬ç“œâ€-->(0,0,1),(0,1,0),(1,0,0).
```


ç›®çš„ï¼šå­¦å¾—f(**x<sub>i</sub>**)=**wx<sub>i</sub>**+bï¼Œä½¿å¾—f(**x<sub>i</sub>**)-->y<sub>i</sub>.

å‚æ•°**w**å’Œbçš„ä¼°è®¡ï¼Œé€šè¿‡f(**x<sub>i</sub>**)-->y<sub>i</sub>æ¥åˆ¤æ–­ã€‚

å¼•è¿›å‡æ–¹è¯¯å·®ï¼š($w^*,b^*$),å¸¦ *çš„æ„æ€æ˜¯($w,b$)çš„è§£ã€‚

#### TIP å¼•è¿›å‡æ–¹è¯¯å·®çš„åŸå› ã€‚

      å‡æ–¹è¯¯å·®çš„çš„å‡ ä½•æ„ä¹‰ä¼˜ç§€ï¼Œå®ƒå¯¹æ¬§æ°è·ç¦»(Eudidean Distance)ã€‚
      åŸºäºå‡æ–¹è¯¯å·®æœ€å°åŒ–è¿›è¡Œæ¨¡å‹æ±‚è§£çš„æ–¹æ³•å³è¢«ç§°ä¸º æœ€å°äºŒä¹˜æ³•(least square method)ã€‚
      åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæœ€å°äºŒä¹˜æ³•å°±æ˜¯è¯•å›¾æ‰¾åˆ°ä¸€æ¡ç›´çº¿ï¼Œä½¿æ‰€æœ‰æ ·æœ¬åˆ°ç›´çº¿ä¸Šçš„æ¬§æ°è·ç¦»è·ç¦»æœ€å°ã€‚


#### ä¸‹é¢æ¨å¯¼ä¸€ä¸‹$d=1$æ—¶ï¼Œæ±‚è§£$w,b$ã€‚

$E_(w,b)$ æ˜¯å…³äº$w,b$çš„å‡¸å‡½æ•°ã€‚å½“å®ƒå…³äº$w,b$çš„å¯¼æ•°ä¸º0æ—¶ï¼Œå¾—åˆ°$w,b$çš„æœ€ä¼˜è§£ã€‚

      ç®€å•è¯´ï¼Œå°±æ˜¯æ•´ä¸ªå‡½æ•°çš„å½¢çŠ¶æ˜¯ä¸ªå¤§å†™çš„Uã€‚äºŒé˜¶å¯¼æ•°éè´Ÿã€‚

$d=1$, åˆ™$f(x)=wx+b$ã€‚

$$E_(w,b)=\sum_{k=1}^m (y_i-wx_i-b)^2$$
å¯çŸ¥
$$\frac{\partial f(x, y)}{\partial w}\ = 2(w \sum_i^m x_i^2-\sum_i^m(y_i-b)x_i)=0$$
$$\frac{\partial f(x, y)}{\partial b}\ = 2(mb-\sum_i^m(y_i-wx_i))=0$$

 ç”±ï¼ˆ2ï¼‰å¼å¾—ï¼š
 $$b=\frac{1}{m}\ \sum_i^m(y_i-wx_i)\$$
 å°†bå¸¦å…¥ï¼ˆ1ï¼‰å¼ï¼š
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ - b\sum_i^mx_i\ = 0$$

 å…¶ä¸­
 $$b\sum_i^mx_i\ = \frac{1}{m}\ \sum_i^m(y_i-wx_i)\sum_i^mx_i\$$
 $$=\frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\$$

 å¸¦å›åˆ°ä¸Šå¼ï¼š
 $$w\sum_i^m x_i^2\ - \sum_i^my_ix_i\ - \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ - \frac{w}{m}\ \sum_i^mx_i^2\ = 0$$
 æå–$w$:
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \frac{1}{m}\ \sum_i^m y_i\ \sum_i^m x_i\ $$

 å…¶ä¸­ï¼š
 $$ \frac{1}{m}\ \sum_i^m x_i\ = \bar{x}$$
 åˆ™
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_ix_i\ + \sum_i^m y_i\ \bar{x}$$
 $$w( \sum_i^m x_i^2\ - \frac{1}{m}\ \sum_i^m x_i^2\ ) = \sum_i^my_i(x_i+\bar{x})\ $$

 å¯æ¨å‡ºï¼š
 



# Logistic Regression/classification çº¿æ€§å›å½’/åˆ†ç±»
ç»™å®šç”±dä¸ªå±æ€§æè¿°çš„å®ä¾‹ **x**=( x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>d</sub> )ï¼Œå…¶ä¸­x<sub>i</sub>æ˜¯**x**åœ¨ç¬¬iä¸ªå±æ€§ä¸Šçš„å–å€¼ã€‚


#### classification åˆ†ç±»
*  $y\in ({0,1})$ï¼Œ **0**:negative class
					 **1**:positive class

*  **y** \in {0,1,2,3}

##### ä¾‹å­ğŸŒ°
